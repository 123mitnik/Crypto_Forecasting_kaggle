{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reference:[https://www.kaggle.com/yamqwe/time-series-modeling-lstm](https://www.kaggle.com/yamqwe/time-series-modeling-lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "##data and date\n",
    "dat = pd.read_csv('./data/train.csv')\n",
    "dat = dat.set_index('timestamp')\n",
    "info = pd.read_csv('./data/asset_details.csv')\n",
    "assets_names = dict(zip(info.Asset_ID, info.Asset_Name))\n",
    "dat['Asset_name'] = dat.Asset_ID.map(assets_names)\n",
    "dat['asset_count'] = 1\n",
    "#should be 14 for each timestamp when consistant\n",
    "dat['asset_count'] = dat.groupby(by = dat.index)['asset_count'].sum()\n",
    "dat.head()\n",
    "all_same_time = dat[dat['asset_count'] == 14]\n",
    "all_same_time = all_same_time.drop('asset_count',axis=1)\n",
    "all_same_time.head()\n",
    "## Target correlation map\n",
    "corr_target = all_same_time.reset_index().pivot(index = 'Asset_name', columns = 'timestamp')['Target'].transpose().corr()\n",
    "fig, ax = plt.subplots(figsize = (20, 8))\n",
    "sns.heatmap(np.round(corr_target, 2), annot = True, ax = ax, square = True)\n",
    "del all_same_time\n",
    "del dat\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd, numpy as np\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "from script.LSTM.configure import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data loading\n",
    "\n",
    "- choose which years to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from script.LSTM.dataload import load_data_for_all_assets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "take asset order into record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load_data_for_all_assets(load_jay = True, includeextra=True)\n",
    "train = train.sort_values('timestamp').set_index(\"timestamp\")\n",
    "if DEBUG: train = train[10000000:]\n",
    "\n",
    "test = pd.read_csv('./data/' + 'example_test.csv')\n",
    "#sample_prediction_df = pd.read_csv('./data/' + 'example_sample_submission.csv')\n",
    "\n",
    "assets = pd.read_csv('./data/asset_details.csv')\n",
    "assets_names = dict(zip(assets.Asset_ID, assets.Asset_Name))\n",
    "train['Asset_Name'] = train.Asset_ID.map(assets_names)\n",
    "assets_weight = dict(zip(assets.Asset_ID, assets.Weight))\n",
    "train['Weight'] = train.Asset_ID.map(assets_weight)\n",
    "\n",
    "#the order of the cryptos collected at same timestamp.\n",
    "assets_order = pd.read_csv('./data/supplemental_train.csv').Asset_ID[:N_ASSETS]\n",
    "assets_order = dict((t,i) for i,t in enumerate(assets_order))\n",
    "print(f\"Loaded all data range {train.index.values.astype('datetime64[s]')[[0,-1]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature encoding\n",
    "\n",
    "- Upper_Shadow, Lower_Shadow,spread,mean_trade,log_price_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from script.LSTM.encodefeature import get_features\n",
    "train['Target'] = train['Target'].fillna(0)\n",
    "VWAP_max = np.max(train[np.isfinite(train.VWAP)].VWAP)\n",
    "VWAP_min = np.min(train[np.isfinite(train.VWAP)].VWAP)\n",
    "train['VWAP'] = np.nan_to_num(train.VWAP, posinf=VWAP_max, neginf=VWAP_min)\n",
    "\n",
    "df = train[['Asset_ID', 'Target']].copy()\n",
    "times = dict((t,i) for i,t in enumerate(df.index.unique()))\n",
    "df['id'] = df.index.map(times)\n",
    "df['id'] = df['id'].astype(str) + '_' + df['Asset_ID'].astype(str)\n",
    "ids = df.id.copy()\n",
    "del df\n",
    "\n",
    "train = get_features(train)\n",
    "train_features = [i for i in train.columns if i not in ['Target', 'date', 'timestamp', 'Asset_ID', 'groups']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##filling missing data\n",
    "train = train.sort_index()\n",
    "ind = train.index.unique()\n",
    "\n",
    "def reindex(df):\n",
    "    df = df.reindex(range(ind[0],ind[-1]+60,60),method='nearest')\n",
    "    df = df.fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "    return df\n",
    "\n",
    "train = train.groupby('Asset_ID').apply(reindex).reset_index(0, drop=True).sort_index()\n",
    "gc.collect()\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matching records and marking generated rows as 'non-real' for masking layer\n",
    "train['group_num'] = train.index.map(times)\n",
    "train = train.dropna(subset=['group_num'])\n",
    "train['group_num'] = train['group_num'].astype('int')\n",
    "train['id'] = train['group_num'].astype(str) + '_' + train['Asset_ID'].astype(str)\n",
    "train['is_real'] = train.id.isin(ids) * 1\n",
    "train = train.drop('id', axis=1)\n",
    "\n",
    "# Features values for 'non-real' rows are set to zeros\n",
    "features = train.columns.drop(['Asset_ID','group_num','is_real'])\n",
    "train.loc[train.is_real == 0, features] = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sort the data by time and asset_id_order\n",
    "from script.LSTM.dataload import reduce_mem_usage\n",
    "train['asset_order'] = train.Asset_ID.map(assets_order)\n",
    "train = train.sort_values(by=['group_num', 'asset_order'])\n",
    "train = reduce_mem_usage(train)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make 3D-numpy arrays for train and targets\n",
    "targets = train['Target'].to_numpy().reshape(-1, N_ASSETS)\n",
    "features = train.columns.drop(['Asset_ID', 'Asset_Name','Weight', 'Target', 'group_num', 'is_real', 'date'])\n",
    "train = train[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.values\n",
    "train = train.reshape(-1, N_ASSETS, train.shape[-1])\n",
    "\n",
    "X_train, X_test = train[:-len(train)//PCT_VALIDATION], train[-len(train)//PCT_VALIDATION:]\n",
    "y_train, y_test = targets[:-len(train)//PCT_VALIDATION], targets[-len(train)//PCT_VALIDATION:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset windowing\n",
    "\n",
    "Samples with a duration of `WINDOW_SIZE` records (minutes) will be formed from the train array. Each sample has a target vector corresponding to the final index if `WINDOW_SIZE` record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from script.LSTM.datasetwindow import sample_generator\n",
    "train_generator = sample_generator(X_train, y_train, length = WINDOW_SIZE, batch_size = BATCH_SIZE)\n",
    "val_generator = sample_generator(X_test, y_test, length = WINDOW_SIZE, batch_size = BATCH_SIZE)\n",
    "print(f'Sample shape: {train_generator[0][0].shape}')#batch_0, window, n_assets, features\n",
    "print(f'Target shape: {train_generator[0][1].shape}')#batch_0, N_ASSETS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling model\n",
    "\n",
    "Our model will be trained for the number of `FOLDS` and `EPOCHS` you chose in the configuration above. Each fold the model with lowest validation loss will be saved and used to predict `OOF`(Out of fold) and test. Adjust the variable `VERBOSE`. The variable `VERBOSE=1 or 2` will display the training and validation loss for each epoch as text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from script.LSTM.trainmodel import get_model\n",
    "\n",
    "model = get_model(n_assets=14,\n",
    "                trainshape=(train_generator[0][0].shape[1], 14, train_generator[0][0].shape[-1]))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(get_model(n_assets=3), show_shapes=True,to_file='./pic/lstm-kaggle.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features)\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "estop = keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 7, verbose = 0, mode = 'min',restore_best_weights = True)\n",
    "scheduler = keras.optimizers.schedules.ExponentialDecay(1e-3, (0.5 * len(X_train) / BATCH_SIZE), 1e-3)\n",
    "lr = keras.callbacks.LearningRateScheduler(scheduler, verbose = 1)\n",
    "\n",
    "history = model.fit(train_generator, validation_data = (val_generator), \n",
    "                    epochs = EPOCHS, callbacks = [lr, estop])\n",
    "model.save(\"kaggleLSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from script.LSTM.trainmodel import *\n",
    "model = keras.models.load_model(\"kaggleLSTM\",custom_objects={\"masked_cosine\": masked_cosine, \n",
    "                                                    'Correlation':Correlation})'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
    "histories = pd.DataFrame(history.history)\n",
    "epochs = list(range(1,len(histories)+1))\n",
    "loss = histories['loss']\n",
    "val_loss = histories['val_loss']\n",
    "Correlation = histories['Correlation']\n",
    "val_Correlation = histories['val_Correlation']\n",
    "ax[0].plot(epochs, loss, label = 'Train Loss')\n",
    "ax[0].plot(epochs, val_loss, label = 'Val Loss')\n",
    "ax[0].set_title('Losses')\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].legend(loc='upper right')\n",
    "ax[1].plot(epochs, Correlation, label = 'Train Correlation')\n",
    "ax[1].plot(epochs, val_Correlation, label = 'Val Correlation')\n",
    "ax[1].set_title('Correlations')\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].legend(loc='upper right')\n",
    "fig.show()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The correlation coefficients by asset for the validation data\n",
    "predictions = model.predict(val_generator)\n",
    "\n",
    "print('Asset:    Corr. coef.')\n",
    "print('---------------------')\n",
    "for i in range(N_ASSETS):\n",
    "    # drop first 14 values in the y_test, since they are absent in val_generator labels\n",
    "    y_true = np.squeeze(y_test[WINDOW_SIZE - 1:, i])\n",
    "    y_pred = np.squeeze(predictions[:, i])\n",
    "    real_target_ind = np.argwhere(y_true!=0)\n",
    "    asset_id = list(assets_order.keys())[i]\n",
    "    asset_name = assets[assets.Asset_ID == asset_id]['Asset_Name'].item()\n",
    "    print(f\"{asset_name}: {np.corrcoef(y_pred[real_target_ind].flatten(), y_true[real_target_ind].flatten())[0,1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission\n",
    "\n",
    "[https://www.kaggle.com/c/g-research-crypto-forecasting/overview/evaluation](https://www.kaggle.com/c/g-research-crypto-forecasting/overview/evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crypto order at same timestamp in the aftergame train data\n",
    "data_path = './data'\n",
    "sup_train=pd.read_csv(data_path+'/supplemental_train.csv')\n",
    "assets_order = sup_train.Asset_ID[:N_ASSETS]\n",
    "assets_order = dict((t,i) for i,t in enumerate(assets_order))\n",
    "features = ['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP',\n",
    "       'Upper_Shadow', 'Lower_Shadow', 'spread', 'mean_trade',\n",
    "       'log_price_change', 'asset_order']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reorder placeholder(supplemental_train.csv) for first 15mins samples(15*14=210 rows)\n",
    "sup = sup_train[:WINDOW_SIZE * (N_ASSETS)]\n",
    "placeholder = get_features(sup)\n",
    "placeholder['asset_order'] = placeholder.Asset_ID.map(assets_order)\n",
    "\n",
    "# shape into our model's input array\n",
    "test_sample = np.array(placeholder[features])\n",
    "test_sample = test_sample.reshape(-1, (N_ASSETS), test_sample.shape[-1])\n",
    "test_sample = np.expand_dims(test_sample, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = pd.read_csv(data_path+'/example_test.csv')[:WINDOW_SIZE - 1]\n",
    "example['asset_order'] = example.Asset_ID.map(assets_order) \n",
    "#test data's asset order\n",
    "example = example[['Asset_ID','asset_order']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    #one minute test sample\n",
    "    test_csv,sample_prediction_df_csv = f'./data/temp/{i}test_df.csv', f'./data/temp/{i}sample_prediction_df.csv'\n",
    "    test_df, sample_prediction_df= pd.read_csv(test_csv,usecols =range(1,11)),pd.read_csv(sample_prediction_df_csv,usecols =range(1,3))\n",
    "    ##add features\n",
    "    test_df = get_features(test_df)\n",
    "    ##sort with asset_order\n",
    "    test_data = test_df.merge(example, how='outer', on='Asset_ID').sort_values('asset_order')\n",
    "    # print(test_data.timestamp)\n",
    "    # print(test_data[features].shape)\n",
    "    ##make into arrays\n",
    "    test = np.array(test_data[features].fillna(0))\n",
    "    print(test.shape)\n",
    "    test = test.reshape(-1, 1, N_ASSETS, test.shape[-1])#add two more axis\n",
    "    print(test.shape)\n",
    "    ##merge with test_sample make 15 minute sample as input to the model\n",
    "    test_input = np.hstack([test_sample, test])[:,-1 * WINDOW_SIZE:]\n",
    "    print(test_input.shape)\n",
    "\n",
    "    # y_pred = model.predict(test_input).squeeze().reshape(-1, 1).squeeze()\n",
    "    # test_data['Target'] = y_pred\n",
    "    # for _, row in test_df.iterrows():\n",
    "    #     try: sample_prediction_df.loc[sample_prediction_df['row_id'] == row['row_id'], 'Target'] = test_data.loc[test_data['row_id'] == row['row_id'], 'Target'].item()\n",
    "    #     except: sample_prediction_df.loc[sample_prediction_df['row_id'] == row['row_id'], 'Target'] = 0\n",
    "    # env.predict(sample_prediction_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fake score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data'\n",
    "sup_train=pd.read_csv(data_path+'/supplemental_train.csv')#fake score source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(4):\n",
    "    print(i)\n",
    "    #one minute test sample\n",
    "    test_csv,sample_prediction_df_csv = f'./data/temp/{i}test_df.csv', f'./data/temp/{i}sample_prediction_df.csv'\n",
    "    test_df, sample_prediction_df= pd.read_csv(test_csv,usecols =range(1,11)),pd.read_csv(sample_prediction_df_csv,usecols =range(1,3))\n",
    "    pred =list(sup_train['Target'][range(i*14,i*14+14)])\n",
    "    pred[3:6] = [0]*3\n",
    "    test_df['Target'] = [round(p,5) for p in pred]\n",
    "    for _, row in test_df.iterrows():\n",
    "        sample_prediction_df.loc[sample_prediction_df['row_id'] == row['row_id'], 'Target'] = test_df.loc[test_df['row_id'] == row['row_id'], 'Target'].item()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4bd624a0593993fe43ac4046b27b898fb2ef75c21c08f81e89e64ea0f51df676"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('tensorflow': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
