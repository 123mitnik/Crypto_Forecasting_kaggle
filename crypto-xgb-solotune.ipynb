{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import talib as ta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import time\n",
    "import xgboost as xgb\n",
    "import os\n",
    "from random import sample\n",
    "from os.path import exists\n",
    "import json\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV   #Perforing grid search\n",
    "from sklearn import metrics\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_MOD_FOLDER = \"./trainedXGB\"#'../input/mytrainedxgb'\n",
    "ASSET_DETAILS_CSV = './data/asset_details.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished loading\n",
      "Index(['timestamp', 'Asset_ID', 'Count', 'Open', 'High', 'Low', 'Close',\n",
      "       'Volume', 'Target', 'Weight', 'lr_15', 'Mkt_lrt_15', 'Crypto_Index',\n",
      "       'beta', 'lr_mkt_resid'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df_asset_details = pd.read_csv(ASSET_DETAILS_CSV).sort_values(\"Asset_ID\")\n",
    "\n",
    "#######\n",
    "df_train=pd.read_feather('./data/new_data.ftr',\n",
    "                        columns=['timestamp', 'Asset_ID', 'Count', 'Open', 'High', 'Low', 'Close',\n",
    "                               'Volume', 'Target', 'Weight', 'lr_15', 'Mkt_lrt_15','Crypto_Index',\n",
    "                                 'beta','lr_mkt_resid'])\n",
    "\n",
    "print('finished loading')\n",
    "print(df_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = df_train[df_train['timestamp']>=df_train['timestamp'].quantile(0.95)]\n",
    "df_retrain = df_train[(df_train['timestamp']>df_train['timestamp'].quantile(0.45)) & \\\n",
    "                      (df_train['timestamp']<df_train['timestamp'].quantile(0.95))]\n",
    "\n",
    "del df_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##initial feature parameters to hyperparameter selection\n",
    "lrtn,fastk1,fastk2,adx,macd_s,macd_l,rsi,std_Crypto_Index,std_lr_15,std_Mkt_lrt_15 = \\\n",
    "(30, 5, 15, 30, 15, 25, 60, 15, 15, 5)\n",
    "\n",
    "def log_return(series, periods=5):\n",
    "    return np.log(series).diff(periods)\n",
    "\n",
    "def upper_shadow(df):\n",
    "    return ta.SUB(df['High'], np.maximum(df['Close'], df['Open']))\n",
    "\n",
    "def lower_shadow(df):\n",
    "    return ta.SUB(np.minimum(df['Close'], df['Open']), df['Low'] )\n",
    "\n",
    "def lag_features(df):    \n",
    "    ####TECH indicators\n",
    "    df['slowK'], df['slowD'] = ta.STOCH(df.High, df.Low, df.Close, \n",
    "                                        fastk_period=fastk1, slowk_period=int(3*fastk1/5), slowd_period=int(3*fastk1/5),\n",
    "                                        slowk_matype=0, slowd_matype=0)\n",
    "    df['fastK'], df['fastD'] = ta.STOCHF(df.High, df.Low, df.Close,\n",
    "                                         fastk_period=fastk2, fastd_period=int(3*fastk2/5), \n",
    "                                         fastd_matype=0)\n",
    "    df[f'rsi_{rsi}'] = ta.RSI(df['Close'], timeperiod=rsi)\n",
    "    df[f'macd_{macd_s}_{macd_l}'],df['macd_signal'], df['macd_hist'] = \\\n",
    "                ta.MACD(df['Close'],fastperiod=macd_s, slowperiod=macd_l, signalperiod=5)\n",
    "    df[f'adx_{adx}'] = ta.ADX(df['High'], df['Low'],df['Close'], timeperiod=adx)#Average Directional Movement Index\n",
    "    df['AD'] = ta.AD(df['High'], df['Low'],df['Close'], df['Volume'])#Accumulation Distribution Line\n",
    "    ####std volatility\n",
    "    df[f'std_lr_15_{std_lr_15}'] = ta.STDDEV(df.lr_15,timeperiod=std_lr_15, nbdev=1)\n",
    "    df[f'std_Mkt_lrt_15_{std_Mkt_lrt_15}'] = ta.STDDEV(df.Mkt_lrt_15,timeperiod=std_Mkt_lrt_15, nbdev=1)\n",
    "    df[f'std_Crypto_Index_{std_Crypto_Index}'] = ta.STDDEV(df.Crypto_Index,timeperiod=std_Crypto_Index, nbdev=1)\n",
    "def get_features(df_feat):\n",
    "    pd.options.mode.chained_assignment = None  # default='warn'\n",
    "    df_feat[f'lrtn_index_{lrtn}'] = log_return(df_feat.Crypto_Index, lrtn)\n",
    "    lag_features(df_feat)\n",
    "    return df_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune start here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish paramx_xgb initialization\n"
     ]
    }
   ],
   "source": [
    "####parameters placeholder\n",
    "#https://xgboost.readthedocs.io/en/stable/parameter.html#general-parameters\n",
    "params_general ={'booster': 'gbtree', 'verbosity':0, 'validate_parameters': 1}\n",
    "params_booster ={\n",
    "    'learning_rate': 0.3,#check\n",
    "    'gamma': 0, #gamma. check\n",
    "    'max_depth': 6,#check\n",
    "    'min_child_weight': 1, #instance weight (hessian). check\n",
    "    'subsample': 0.8,#check\n",
    "    'colsample_bytree': 1,#check\n",
    "    'reg_lambda': 1,#L2 regularization term on weights\n",
    "    'reg_alpha': 0, #L1 regularization term on weights\n",
    "    'tree_method': 'hist', #hist, gpu_hist\n",
    "    'predictor': 'auto', #auto, gpu_predictor\n",
    "}\n",
    "params_learning={\n",
    "    'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'base_score':0.5,\n",
    "    'seed': 2021\n",
    "}\n",
    "\n",
    "params_xgb = {**params_general, **params_booster, **params_learning}\n",
    "\n",
    "#https://xgboost.readthedocs.io/en/stable/parameter.html#command-line-parameters\n",
    "params_train={\n",
    "    'num_boost_round': 500, #alias as 'n_estimators' in sklearn api\n",
    "    'early_stopping_rounds':10, 'verbose_eval': False\n",
    "}\n",
    "\n",
    "############alias in sklearn api\n",
    "params_sklearn={'n_estimators':params_train['num_boost_round']}\n",
    "xgb_sklearn = XGBRegressor(**params_xgb,**params_sklearn)\n",
    "\n",
    "print(\"finish paramx_xgb initialization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective': 'reg:squarederror',\n",
       " 'base_score': 0.5,\n",
       " 'booster': 'gbtree',\n",
       " 'colsample_bylevel': None,\n",
       " 'colsample_bynode': None,\n",
       " 'colsample_bytree': 1,\n",
       " 'enable_categorical': False,\n",
       " 'gamma': 0,\n",
       " 'gpu_id': None,\n",
       " 'importance_type': None,\n",
       " 'interaction_constraints': None,\n",
       " 'learning_rate': 0.3,\n",
       " 'max_delta_step': None,\n",
       " 'max_depth': 6,\n",
       " 'min_child_weight': 1,\n",
       " 'missing': nan,\n",
       " 'monotone_constraints': None,\n",
       " 'n_estimators': 500,\n",
       " 'n_jobs': None,\n",
       " 'num_parallel_tree': None,\n",
       " 'predictor': 'auto',\n",
       " 'random_state': None,\n",
       " 'reg_alpha': 0,\n",
       " 'reg_lambda': 1,\n",
       " 'scale_pos_weight': None,\n",
       " 'subsample': 0.8,\n",
       " 'tree_method': 'hist',\n",
       " 'validate_parameters': 1,\n",
       " 'verbosity': 0,\n",
       " 'eval_metric': 'rmse',\n",
       " 'seed': 2021}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_sklearn.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make tune data\n",
    "def make_tune_data(df, asset_id=1):\n",
    "    pd.options.mode.chained_assignment = None  # default='warn'\n",
    "    tune_train = df_retrain[df_retrain[\"Asset_ID\"] == asset_id]\n",
    "    tune_train = get_features(tune_train)\n",
    "    tune_train.dropna(axis = 0, inplace= True)#for lag_features missing rows:<100\n",
    "    dtrain=xgb.DMatrix(tune_train.drop(['timestamp', 'Asset_ID','Target','Weight'],axis=1),\n",
    "                    label= tune_train['Target'])\n",
    "    ###########\n",
    "    tune_test = df_test[df_test[\"Asset_ID\"] == asset_id]\n",
    "    tune_test = get_features(tune_test)\n",
    "    tune_test.dropna(axis = 0, inplace= True)#for lag_features missing rows:<100\n",
    "    dtest=xgb.DMatrix(tune_test.drop(['timestamp', 'Asset_ID','Target','Weight'],axis=1),\n",
    "                    label= tune_test['Target'])\n",
    "    return tune_train,dtrain,tune_test,dtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_train,dtrain,tune_test,dtest = make_tune_data(df=df_retrain, asset_id=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelfit(xgb_sklearn, tune_train, dtrain, useTrainCV=True, cv_folds=5, early_stopping_rounds=10):\n",
    "    '''\n",
    "    useTrainCV: is for auto select best n_estimators or num_boost_round.\n",
    "    '''\n",
    "    if useTrainCV:\n",
    "        xgb_param = xgb_sklearn.get_xgb_params()\n",
    "        cvresult = xgb.cv(xgb_param, dtrain, \n",
    "                          num_boost_round = xgb_sklearn.get_params()['n_estimators'], \n",
    "                          nfold=cv_folds, \n",
    "                          metrics=['rmse'], \n",
    "                          early_stopping_rounds=early_stopping_rounds,\n",
    "                          verbose_eval=False,as_pandas=True)\n",
    "        xgb_sklearn.set_params(n_estimators = cvresult.shape[0])\n",
    "    #Fit the algorithm on the data\n",
    "    xgb_sklearn.fit(X=tune_train.drop(['timestamp', 'Asset_ID','Target','Weight'],axis=1),\n",
    "                    y= tune_train['Target'], eval_metric='rmse')\n",
    "    #Predict training set:\n",
    "    dtest_predictions = xgb_sklearn.predict(tune_test.drop(['timestamp', 'Asset_ID','Target','Weight'],axis=1))\n",
    "    dtrain_predictions = xgb_sklearn.predict(tune_train.drop(['timestamp', 'Asset_ID','Target','Weight'],axis=1))\n",
    "    print(f\"test-set-mse: {metrics.mean_squared_error(y_true=tune_test['Target'],y_pred=dtest_predictions)}, \\\n",
    "           train-set-mse: {metrics.mean_squared_error(y_true=tune_train['Target'],y_pred=dtrain_predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test-set-mse: 2.1307763342596816e-05,            train-set-mse: 2.4657125145904318e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'objective': 'reg:squarederror',\n",
       " 'base_score': 0.5,\n",
       " 'booster': 'gbtree',\n",
       " 'colsample_bylevel': 1,\n",
       " 'colsample_bynode': 1,\n",
       " 'colsample_bytree': 1,\n",
       " 'enable_categorical': False,\n",
       " 'gamma': 0,\n",
       " 'gpu_id': -1,\n",
       " 'importance_type': None,\n",
       " 'interaction_constraints': '',\n",
       " 'learning_rate': 0.3,\n",
       " 'max_delta_step': 0,\n",
       " 'max_depth': 6,\n",
       " 'min_child_weight': 1,\n",
       " 'missing': nan,\n",
       " 'monotone_constraints': '()',\n",
       " 'n_estimators': 500,\n",
       " 'n_jobs': 8,\n",
       " 'num_parallel_tree': 1,\n",
       " 'predictor': 'auto',\n",
       " 'random_state': 2021,\n",
       " 'reg_alpha': 0,\n",
       " 'reg_lambda': 1,\n",
       " 'scale_pos_weight': 1,\n",
       " 'subsample': 0.8,\n",
       " 'tree_method': 'hist',\n",
       " 'validate_parameters': 1,\n",
       " 'verbosity': 0,\n",
       " 'eval_metric': 'rmse',\n",
       " 'seed': 2021}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelfit(xgb_sklearn, tune_train, dtrain)\n",
    "xgb_sklearn.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gsearch_tune(grid_dict, xgb_sklearn):\n",
    "    gsearch = GridSearchCV(estimator = XGBRegressor(**xgb_sklearn.get_params()), \n",
    "                        param_grid = grid_dict, scoring='neg_root_mean_squared_error', cv=5, refit=True)\n",
    "    gsearch.fit(tune_train.drop(['timestamp', 'Asset_ID','Target','Weight'],axis=1),\n",
    "             tune_train['Target'])\n",
    "    print(f\"gsearch.best_score_: {gsearch.best_score_}, gsearch.best_params_: {gsearch.best_params_}\")\n",
    "    return gsearch.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### max_depth, min_child_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##around optimal check\n",
    "param_test = {\n",
    " 'max_depth':[5,8,10],\n",
    " 'min_child_weight':[0.8,1,1.5,2,2.2]\n",
    "}\n",
    "xgb_sklearn = gsearch_tune(grid_dict=param_test, xgb_sklearn = xgb_sklearn)\n",
    "# modelfit(xgb_sklearn, tune_train, dtrain)\n",
    "# xgb_sklearn.get_params()['n_estimators']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##around optimal check\n",
    "param_test = {\n",
    " 'gamma':[0,0.001]\n",
    "}\n",
    "xgb_sklearn = gsearch_tune(grid_dict=param_test, xgb_sklearn = xgb_sklearn)\n",
    "# modelfit(xgb_sklearn, tune_train, dtrain)\n",
    "# xgb_sklearn.get_params()['n_estimators']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### subsample and colsample_bytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##around optimal check\n",
    "param_test = {\n",
    " 'subsample':[0.6,0.7,0.8,0.9],\n",
    " 'colsample_bytree':[0.6,0.7,0.8,0.9]\n",
    "}\n",
    "xgb_sklearn = gsearch_tune(grid_dict=param_test, xgb_sklearn = xgb_sklearn)\n",
    "#modelfit(xgb_sklearn, tune_train, dtrain)\n",
    "#xgb_sklearn.get_params()['n_estimators']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##around optimal check\n",
    "param_test = {\n",
    " 'reg_alpha':[0, 0.001, 0.01, 0.1,1],\n",
    " 'reg_lambda':[1,1.2,1.5,2]\n",
    "}\n",
    "xgb_sklearn = gsearch_tune(grid_dict=param_test, xgb_sklearn = xgb_sklearn)\n",
    "# modelfit(xgb_sklearn, tune_train, dtrain)\n",
    "# xgb_sklearn.get_params()['n_estimators']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##around optimal check\n",
    "param_test = {\n",
    " 'learning_rate':[0.3, 0.5, 0.7]\n",
    "}\n",
    "xgb_sklearn = gsearch_tune(grid_dict=param_test, xgb_sklearn = xgb_sklearn)\n",
    "# modelfit(xgb_sklearn, tune_train, dtrain)\n",
    "# xgb_sklearn.get_params()['n_estimators']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelfit(xgb_sklearn, tune_train, dtrain)\n",
    "# xgb_sklearn.get_params()['n_estimators']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_sklearn.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_xgb.update(xgb_sklearn.get_xgb_params())\n",
    "params_train['num_boost_round'] = xgb_sklearn.get_params()['n_estimators']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_correlation(a, b, weights):\n",
    "  w = np.ravel(weights)\n",
    "  a = np.ravel(a)\n",
    "  b = np.ravel(b)\n",
    "  sum_w = np.sum(w)\n",
    "  mean_a = np.sum(a * w) / sum_w\n",
    "  mean_b = np.sum(b * w) / sum_w\n",
    "  var_a = np.sum(w * np.square(a - mean_a)) / sum_w\n",
    "  var_b = np.sum(w * np.square(b - mean_b)) / sum_w\n",
    "  cov = np.sum((a * b * w)) / np.sum(w) - mean_a * mean_b\n",
    "  corr = cov / np.sqrt(var_a * var_b)\n",
    "  return corr\n",
    "\n",
    "def make_testset(df):\n",
    "    ###consistent timestamp for all 14 assets\n",
    "    df2 = df.set_index(\"timestamp\").copy()\n",
    "    ind = df2.index.unique()\n",
    "    def reindex(df):\n",
    "        df = df.reindex(range(ind[0],ind[-1]+60,60),method='nearest')\n",
    "        df = df.fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "        return df\n",
    "    df2 = df2.groupby('Asset_ID').apply(reindex).reset_index(0, drop=True).sort_index()\n",
    "    ###add features\n",
    "    df2 = df2.groupby('Asset_ID').apply(lambda x: get_features(x))\n",
    "    return df2.dropna(axis = 0).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_for_asset(df_train, df_val,asset_id):\n",
    "    pd.options.mode.chained_assignment = None  # default='warn'\n",
    "    dftrain = df_train[df_train[\"Asset_ID\"] == asset_id].copy()\n",
    "    #dfval = df_val[df_val[\"Asset_ID\"] == asset_id]\n",
    "    dftrain = get_features(dftrain)\n",
    "    #dfval = get_features(dfval)\n",
    "    dftrain.dropna(axis = 0, inplace= True)\n",
    "    #dfval.dropna(axis = 0, inplace= True)\n",
    "    #https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.DMatrix\n",
    "    dmat_train=xgb.DMatrix(data = dftrain.drop(['timestamp', 'Asset_ID','Target','Weight'],\n",
    "                                           axis=1),\n",
    "                       label= dftrain['Target'])\n",
    "    #dmat_val=xgb.DMatrix(data = dfval.drop(['timestamp', 'Asset_ID','Target','Weight'],axis=1),label= dfval['Target'])\n",
    "    del dftrain#,dfval\n",
    "    gc.collect()\n",
    "\n",
    "    #https://xgboost.readthedocs.io/en/stable/python/python_api.html#module-xgboost.training\n",
    "    evals_result = {}\n",
    "    model = xgb.train(params_xgb, dtrain=dmat_train, \n",
    "                      evals=[(dmat_train,'train')],#[(dmat_val,'val')],\n",
    "                      #feval = pearson_metric, evals_result=evals_result,\n",
    "                      **params_train)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def model_reload_train(df_train, df_val):\n",
    "    models = {}\n",
    "    for asset_id, asset_name in zip(df_asset_details['Asset_ID'], df_asset_details['Asset_Name']):\n",
    "        print(f\"training model for {asset_name:<16} (ID={asset_id:<2})\")\n",
    "        models[asset_id] = train_model_for_asset(df_train, df_val,asset_id)\n",
    "        #models[asset_id].save_model(f'./model_nof_{params_version}/model_{asset_id}.json')\n",
    "    return models\n",
    "\n",
    "models=model_reload_train(df_train=df_retrain, \n",
    "                   df_val=df_test)\n",
    "\n",
    "print(models[0].save_config())\n",
    "print(models[0].feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_importance(models[0])#trees are decision stumps with depth 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "score by weighted correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = make_testset(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_frame = []\n",
    "for id in range(0,14):\n",
    "    model = models[id]\n",
    "    x = df_test[df_test['Asset_ID']==id]\n",
    "    x['Pred'] = model.predict(xgb.DMatrix(x[model.feature_names]))\n",
    "    result_frame.append(x[['timestamp','Asset_ID','Weight','Target','Pred']])\n",
    "result = pd.concat(result_frame, axis=0)\n",
    "########################################\n",
    "score=weighted_correlation(a=result['Target'], \n",
    "                    b=result['Pred'], \n",
    "                    weights=result['Weight'])\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4c1b049caad8a8b325ed882a549e2be8617c954355cf49088b2db08ccb0041d6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('myML': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
