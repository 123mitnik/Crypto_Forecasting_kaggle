{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network forecasting\n",
    "\n",
    "- Reference:`keras.layers.SimpleRNN`\n",
    "\n",
    "**Basic Architecture diagram**  \n",
    "\n",
    "<img src=\"./pic/RNN.png\" alt=\"RNN Architecture\" width=\"500\"/>\n",
    "\n",
    "- U,V,W are weight matrix.\n",
    "- Input: vector $X_t$ is input for network at time step t.  \n",
    "- Hidden state: vector $h(t)=\\tanh \\left(W h(t-1)+U_{x}(t)\\right)$\n",
    "- Output: $y_t$ is the output for the network at time step t. $y_t=\\operatorname{softmax}(V s(t))$\n",
    "\n",
    "Every neuron is assigned to a fixed step. The output of the hidden layer of one time step is part of the input of next time step.\n",
    "\n",
    "- The algorithm is to find the optimal weight matrix U,V,W that gives the best prediction or minimizes the loss function $J$. \n",
    "$$J(\\theta)=\\frac{1}{m} \\sum_{i=1}^{m} \\sum_{t=1}^{N_{i}} D(y_t, Y_t)$$\n",
    "- Forward/Backward propagation\n",
    "\n",
    "## Vanishing gradiant problems\n",
    "\n",
    "- LSTM `keras.layers.LSTM`, first proposed in\n",
    "[Hochreiter & Schmidhuber, 1997](https://www.bioinf.jku.at/publications/older/2604.pdf).\n",
    "- GRU `keras.layers.GRU`, first proposed in\n",
    "[Cho et al., 2014](https://arxiv.org/abs/1406.1078)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Crypto-forcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import IPython\n",
    "import IPython.display\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "mpl.rcParams['figure.figsize'] = (8, 6)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "#tf.debugging.set_log_device_placement(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##data and date\n",
    "csv_path = '/Users/dingxian/Documents/GitHub/Crypto_Forecasting_kaggle/codetest/btc.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "df = df[-70000:]\n",
    "date_time = pd.to_datetime(df.pop('timestamp'),unit='s')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data\n",
    "\n",
    "split for the training, validation, and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Count','Open','High','Low','Close','Volume','VWAP']]\n",
    "column_indices = {name: i for i, name in enumerate(df.columns)}\n",
    "\n",
    "n = len(df) #rows\n",
    "train_df = df[0:int(n*0.7)]\n",
    "val_df = df[int(n*0.7):int(n*0.9)]\n",
    "test_df = df[int(n*0.9):]\n",
    "\n",
    "num_features = df.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize the data\n",
    "scale features before training a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean = train_df.mean()\n",
    "train_std = train_df.std()\n",
    "\n",
    "train_df = (train_df - train_mean) / train_std\n",
    "val_df = (val_df - train_mean) / train_std\n",
    "test_df = (test_df - train_mean) / train_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data windowing\n",
    "\n",
    "The models make a set of predictions based on a window of consecutive samples from the data.\n",
    "- The `width` (number of time steps) of the `input` and `label` windows.\n",
    "- The `time offset` between them.\n",
    "- Which features are used as `inputs`, `labels`, or both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from script.RNN.window import WindowGenerator\n",
    "wide_window = WindowGenerator(\n",
    "    input_width=30, label_width=30, shift=1,train_df=train_df, val_df=val_df, test_df=test_df,\n",
    "    label_columns=['Close'])\n",
    "\n",
    "wide_window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long Short-Term Memory  \n",
    "<img src=\"./pic/LSTM-1.png\" alt=\"LSTM Architecture\" width=\"500\"/> \n",
    "\n",
    "- ` tf.keras.layers.LSTM`\n",
    "- `return_sequence=True`, it will return something with shape: `(batch_size, timespan, unit)`. \n",
    "- `return_sequence=False`, then it just return the last output in shape `(batch_size, unit)`.\n",
    "\n",
    "\n",
    "### Model Design\n",
    "- `LSTM(units = 32)`: LSTM layer with 32 internal units.\n",
    "- `Dense(units=1)`: Dense layer with 1 units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(units = 32, return_sequences=True),#Recurrent layers\n",
    "    tf.keras.layers.Dense(units=1)#densely-connected NN layer\n",
    "])\n",
    "IPython.display.clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Input shape:', wide_window.example[0].shape)#[batch, timesteps, feature]\n",
    "print('Output shape:', lstm_model(inputs= wide_window.example[0]).shape)\n",
    "print('Label shape:', wide_window.example[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_performance={}\n",
    "performance={}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting\n",
    "- `compile_and_fit()`: do `model.compile`, `model.fit`, `TensorBoard`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension.\n",
    "%load_ext tensorboard\n",
    "import tensorboard\n",
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from script.RNN.compilefit import compile_and_fit\n",
    "history = compile_and_fit(model = lstm_model, window = wide_window, \n",
    "                            patience=2,MAX_EPOCHS = 2)\n",
    "\n",
    "val_performance['LSTM'] = lstm_model.evaluate(wide_window.val)\n",
    "performance['LSTM'] = lstm_model.evaluate(wide_window.test, verbose=0)\n",
    "#IPython.display.clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization the architecture\n",
    "- `lstm_model.summary()`\n",
    "- Graphs dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Op-level graph\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_window.plot(lstm_model)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c6e4e9f98eb68ad3b7c296f83d20e6de614cb42e90992a65aa266555a3137d0d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
