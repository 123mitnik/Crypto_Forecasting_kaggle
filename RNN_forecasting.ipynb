{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network forecasting\n",
    "\n",
    "- Reference:[https://towardsdatascience.com/time-series-forecasting-with-deep-learning-and-attention-mechanism-2d001fc871fc](https://towardsdatascience.com/time-series-forecasting-with-deep-learning-and-attention-mechanism-2d001fc871fc)\n",
    "\n",
    "**Architecture diagram**\n",
    "![RNN Architecture](./pic/RNN.png)\n",
    "\n",
    "- U,V,W are weight matrix.\n",
    "- Input: vector $X_t$ is input for network at time step t.  \n",
    "- Hidden state: vector $h(t)=\\tanh \\left(W h(t-1)+U_{x}(t)\\right)$\n",
    "- Output: $y_t$ is the output for the network at time step t. $y_t=\\operatorname{softmax}(V s(t))$\n",
    "\n",
    "Every neuron is assigned to a fixed step. The output of the hidden layer of one time step is part of the input of next time step.\n",
    "\n",
    "- The algorithm is to find the optimal weight matrix U,V,W that gives the best prediction or minimizes the loss function $J$. \n",
    "$$J(\\theta)=\\frac{1}{m} \\sum_{i=1}^{m} \\sum_{t=1}^{N_{i}} D(y_t, Y_t)$$\n",
    "- Forward/Backward propagation"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c6e4e9f98eb68ad3b7c296f83d20e6de614cb42e90992a65aa266555a3137d0d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
